right panel

안녕하세요. 저희는 폴라에서 사용하고 있는 흔들림보정기능을 설명드리고 있습니다.
이쪽에 보시면 알고리즘이 동작하는 과정을 확인하실수 있는데요, 우선 스마트폰은 사람 손으로 들고 촬영을 하기 때문에
연속촬영을해서 GIF로 만들면 보시는것과 같이 흔들림이 존재하는데 이것을 두번째 보시는것과 같이
특징점이라는것을 추출하고, 이 특징점중에 공통적으로 추출되는것을 매칭해서 카메라 파라메터를 구한 뒤에 보정을해주면
네번째 보이는것과같이 흔들림이 보정된 이미지를 얻을 수 있게됩니다.

폴라에서 흔들림보정기능을 사용해보시려면 이쪽의 네단계를 거치면 되는데요,
우선 다운을 받으시고 실행시키시면 아랫쪽에 카메라버튼이 보입니다.
카메라에서 루픽버튼을 누르고 흔들림을 최소로해서 촬영을 하고 나면 보정버튼이 보이는데
이 보정버튼을 누르면 떨림이 보정된 이미지를 얻을 수 있습니다.

이렇게 개발된 코드는 오픈소스화해서 깃헙에서 확인하실수 있습니다.

감사합니다.


Left panel

우선은 루픽이라는 기능에 대해 설명해드려야하는데, 요즘 움짤이 대세이다보니
폴라에서도 자체적으로 움짤을 생산 할 수 있는 기능이 있으면 좋겠다고 생각해서 제작된 기능이고
0.25초 간격으로 6장의 사진을 연속으로 촬영한 뒤에 보시는것과 같이 에니메이션GIF형태로 만들어주는 기능입니다.

헌데 이 기능을 만들고나서 한가지 불만족스러운부분이 있었는데 지금 보시는것과같이 떨림이 심하다는겁니다.
아무래도 사람손으로 스마트폰을 들고서 0.25초 간격으로 촬영을했기때문에 보시는것과같이 어느정도의 떨림이 있을 수 밖에 없었습니다.

그래서 이 떨림을 보정할수 있는 기능에 대해서 알아보기 시작했는데, 우선 하드웨어적인 방법들을 쉽게 찾을 수 있었습니다.
DSLR과 같은 카메라에서 사용되는 손떨림보정기능은 자이로센서와 같은 기울기를 측정할 수 있는 센서를 이용해서 
기울어지는 방향과 반대방향으로 앞부분의 렌즈부분을 움직여주거나 뒷부분의 센서를 움직여주는 방식으로 보정을 합니다.

소프트웨어적으로도 비슷하게 구현하면 되지않을까 생각을했습니다.
이미지가 떨린다고하면 떨리는것의 반대방향으로 이미지를 이동시켜주면되고, 얼마나 이동했는지는 이전 이미지와 비교해보면 알 수 있지 않을까 생각할 수 있습니다.

프로세스를 정리해보면
우선 이미지에서 특징점이라는 것을 뽑습니다. 특징점은 주변 픽셀과 비교해봤을 때 눈에띄게 다른 값을 가지는 픽셀이라고 생각하시면 됩니다.
이렇게 프레임별로 구한 특징점들 중 같은 특징점을 찾는 과정을 거치고,
같은 특징점을 찾고나면 그 특징점들의 2D 이동을 하게하는 카메라의 3D적인 이동을 추정해내는 작업을 하게됩니다.

특징점을 뽑는다는것은 직소퍼즐 맞출때를 상상하면 이해하기 쉬운데요, 오른쪽의 A,B,C라는 퍼즐이 있으면 한눈에봐도 어디 위치할 수 있는 C라는 퍼즐을 먼저 맞출것이고, 그 뒤에 줄무늬때문에 벽의 어디라는것과 회전을 알 수 있는 B라는 것을 맞출것이고 마지막으로 색때문에 하늘의 어디라는 것을 알 수 있지만 퍼즐내에 색의 변화가 없어서 회전이나 위치를 예상할 수없는 A라는 것을 마지막에 맞출것입니다. 여기서 얘기하고 싶은건 결국 이미지의 특정부분은 다른부분에 비해 구분하기가 쉬운 특징이 있는 부분이 있고, 그런부분을 특징점이라고 생각 할 수 있다는 것입니다. 
특징점을 추출하는데는 굉장히 많은 방법이 있는데, 모바일에서 사용하기에 충분히 빠른 방법중에 하나가 FAST라는 방법입니다. FAST라는 방법은 p라는 점이 특징점이 되려고하면 p 라는 점에서 3픽셀 간격에 있는 16개의 점들과 비교를 해봤을 때 일정 Threashold값 이상으로 차이가나고, 그런점들이 9개나, 12개 이상이 되면 그 점을 특징점이라고 볼 수 있다는 것입니다. 

이 방식을 이용해서 특징점을 추출해보면 지금 보시는것과 같이 색의 변화가 심한 부분에 특징점이 추출되는 것을 확인 할 수 있습니다
이렇게 특징점을을 추출하고나면 다음으로 이런 특징점들을 매칭하는 작업을 해야합니다.

특징점을 매칭하기위해선 Descriptor라는 것을 알아야하는데 Descriptor는 특징점과 주변점과의 관계를 기술해놓은 자료구조라고 생각하시면 됩니다. 지금 보시는것은 SIFT라는 것의 Descriptor로 특징점 주변의 16*16 픽셀들의 Gradient를 구해서 8방향의 벡터 중 어디에 속하는지를 파악합니다. 이렇게 하면 총 128차원의 벡터가 생성이되는데 이 128차원을 비교해봄으로써 검출된 특징점이 같은 점인지 아닌지를 알게됩니다. 그런데 이 SIFT는 특허문제도 있고 메모리적으로도 모바일에서 사용하기는 무리가 있습니다. 그래서 BRISK같은 이진Descriptor를 사용하는데 특징점 주변의 점 60개를 일정간격으로 샘플링한 뒤에 이 점들간의 조합으로 디스크립터를 구하는 방식입니다. 이렇게 디스크립터를 구하고 빠르게 매칭을 하기위한 Kd-tree 같은 자료구조들도 필요합니다.
이렇게 매칭하는 작업을하고나면 오른쪽보시는것과같이 점들이 일관되게 나오는 것을 확인할 수 있습니다.
이렇게 매칭되는 점들을 찾고나면 이 점들의 2D 이동들을 가지고 실제 3D상에서 카메라가 어떻게 이동했는지를 찾는 작업이 필요합니다.
Ransac과 같은 알고리즘을 이용해서 이 파라메터를 찾고 이미지를 보정해주는 작업을 하고나면 지금보시는것과 같이 떨림이 보정된 이미지를 얻을 수 있습니다.

헌데 여기까지의 알고리즘은 한가지 문제점이 있습니다. 화면에 움직이는 물체가 있다면 그 움직이는 물체의 움직임까지 보정에 반영이 될 건데 그렇기 때문에 이런 움직이는 물체에서 추출된 특징점을 제외시키는 작업이 필요합니다. 여기에도 다양한 알고리즘이 있는데 일단은 간단하게 일정쓰레시홀드값이상으로 움직이는 점들을 제거하고 공통적으로 잡히는 점들만 남겨보면 다음과 같은 결과를 얻을 수 있습니다.
이런 점들만 가지고 보정을하면 오른쪽과 같은 보정된 결과를 얻을 수 있습니다.

이런게 만든 코드들은 오픈소스화해서 깃헙을 통해 접근할 수 있도록 공개했습니다.