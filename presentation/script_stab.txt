안녕하세요 폴라개발 전은철입니다.
간단하게 제 소개를 하면 저는 올해 3월부터 네이버에 들어와서 폴라 개발을 시작했고,
원래 영상처리를 하던 사람도 아니고, 폴라앱이 영상처리와 많이 관련된 앱도 아닌데,
오늘 발표는 연속촬영된 이미지들을 보정하는 영상처리 관련된 내용을 하게 되었는데요.
그러다보니 영상처리 관련해서 깊이 있는 내용을 얘기할 건 아니구요,
그냥 영상처리쪽이 생각보다는 쉽구나. 잘 모르는데 어떤 프로젝트 할 때
영상처리쪽 기술이 필요하면 내가 간단하게 짜봐도 되겠구나 하는 생각이 들었으면
좋겠다고 생각하면서 만든 발표자료입니다.
아무튼 발표내용은 루픽 손떨림방지 기능개발에 대한 내용입니다.

우선 루픽이라는 기능에 대해서 설명을 드려야하는데,
요즘 움짤이 대세다보니 움짤을 만들어줄 수 있는 기능을 넣어보자 해서 폴라에 들어간 기능이구요,
0.25초 간격으로 7장의 사진을 찍은 후에 보시는것과 같이 animated GIF로 만들어주는 기능입니다.

예.. 익숙한 곳이죠... 근데 루픽 개발이 끝나고 신나게 찍고 다니는데
약간 불만족스러운 부분이 있더라구요...
예 떨림이 심한건데요.. 아무래도 사람이 들고 찍고, 0.25초 간격으로 사진을 찍다보니
흔들림이 심할 수 밖에 없고... 특히 저는 저희팀에서 사진을 못찍는 사람 중 하난데,
원래 한장을 찍어도 흔들리는 사람이에요.. 그러다보니 손떨림을 꼭 넣어야겠다 싶었구요.

그래서 설떨림 방지 기능을 알아보고 있었는데, 많은 솔루션들이 있어요.
렌즈시프트라고 해서 자이로로 기울기를 받고 기울기의 반대로 렌즈를 움직여주는 방식도 있고,
왼쪽 그림처럼 센서를 움직여주는 방식도 있습니다.
이렇게 하드웨어적인 방식들도 있고, 촬영된 동영상에서 떨림을 제거하는
소프트웨어적인 방식도 있는데, 소프트웨어적인 방식은 패키지로 제공되는 솔루션들이 좀 있는거 같긴 한데,
제가 잘 못찾은걸수도 있는데 공개된 소스가 거의 없더라구요.
그래서 개발에 들어갔습니다.

우선 개발하기전에 제약사항을 파악을 했는데,
몇가지 요소가 있었어요
알고리즘이 실시간으로 처리되어야 하는가, 장수가 굉장히 많아서 전체를 가지고 처리하기 힘들고, 한장한장씩 이터레이티브하게
처리를해야하는가. 떨림정도가 어느정도인가..개발기간은 얼마나 있는가 정도였는데요.

우선 이 알고리즘이 빠르면 좋긴한데 실시간으로 처리될 필요까지는 없었어요. 이미지를 촬영한 후에 필터를 입히고, GIF로 만드는
과정에서 이미 프로그레스바를 띄워주고 있는 상황이라 사용자가 참을 수 있을정도의 속도면 괜찮다고 생각했습니다.

이터레이티브한지도, 장수가 7장밖에 안되기때문에, 동영상처럼 1초에 30장씩 나오는 이미지들을 처리를 하는 것 보다 훨씬 쉬운 상황이었구요.

떨림은 애초에 이 루픽이라는건 사용자가 손을 떨지 않는다는 생각을 하고 촬영을 하는 것이고,
떨림이 심할때는 그냥 손떨림을 제외시키면 되는 상황이라 크게 고려하지 않아도 되었구요.

개발기간은 이건 스펙에 있었던것도 아니고, 그냥 그런 기능도 있었으면 좋겠다 하는 기능이기 때문에
아무도 신경을 안쓰고 있었습니다. 그냥 개발해보고 안되면 아 시간이 좀 많이 걸리겠네요. 하면 되는 상황이었죠.

이렇게 제약사항을 파악하고 나니 굳이 개발을 못할 이유가 없었어요.
그래서 최대한 간단하게 구현해보자. 기존의 일을 하면서 가끔 시간이 날때 해보고 안되면 말지 하는 마음으로 시작했는데요.
지금 발표자료 보시면 최대한 간단하게 구현해보자는 글자가 다른 글자들에 비해 급 작아졌죠.
시작의 마음은 이랬는데, 실제 개발을 끝내고나서 생각은 간단하게 구현하는건 아는것이 많을때나 가능한거고,
아는게 없는 상황에서는 시행착오를 하고, 복잡하게 구현이 될 수 밖에 없다는걸 다시 한번더 깨닳은...

아무튼 여기까지는 서론이었고, 이제 실제 알고리즘의 프로세스를 구상하는 단계입니다.

문제인 이미지가 떨린다. 이 문제 자체는 아까 하드웨어적인방법에서 랜즈가 움직이던 센서가 움직이던
얼마나 움직였는지를 파악해서 되돌려주면 되지 않는가 하는거죠.
그럼 얼마나 이동했는지는 어떻게 아는가? 그건 뭐 전장에서 나왔던 부분이 다음장에서 어디에 위치하는가를 보면
알수 있지 않을까 싶었습니다. 간단하죠.

정리를 해보면
우선 각 이미지에서 특징점이라는것을 뽑습니다.
일단 특징점이라는건 이미지들에서 이 부분은 주변 점들과 비교했을 때 구분이 되는 점입니다.
다음으로 이전이미지와 비교를해서 이전 이미지와 현재 이미지에 공통적으로 나타난 특징점을 뽑아냅니다.
특징점이란건 안뽑히는거에서는 몇십개, 진짜 단색의 이미지에서는 아예 안뽑히고, 많이 뽑히는 곳에서는 이삼천개씩 뽑히기 때문에
뽑힌 특징점에서 같은 특징점이란걸 찾는 과정이 필요합니다.
이 과정 이후에 같은 특징점을 찾았으면, 이미지 평면상에서 좌 우로 얼마나 움직였으니, 실제 촬영했던 카메라 자체는
3차원에서 어떻게 움직였는지를 예측해내는 과정이 있게 됩니다.
단순히 생각해도 카메라가 줌인 줌아웃 됬다고 하면 왼쪽에 있던 점들은 오른쪽으로 움직였을 것이고,
오른쪽에 있던 점들은 왼쪽으로 움직이게 되겠죠. 어떤 점들이 오른쪽으로 얼마만큼 이동했다고 단순히 카메라가 오른쪽으로 이동했다고
생각할 수는 없는겁니다. 그래서 뽑혀진 점들에서 카메라의 이동파라메터를 찾는 추정과정이 있게됩니다.

다음으로 각 세부파트에 대해서 설명하기 전에 주의사항으로,
지금부터 설명하는 프로세스는 최선이 아닐 수 있습니다.
더 좋은 방법이 있는데 제가 찾지 못했을 수도 있고, 시간이 지나게 되면 당연히 더 좋은 방법이 나올거구요,
이 방법은 제가 단순히 빨리 생각난걸 구현해보고, 구현해봤더니 이런 케이스에선 나쁘지 않은 성능을 보여주더라 하는거고
문제 해결에 집중하기 보다는 저렇게 푸네... 저정도는 나도 하겠는데 에 집중해주세요


특징점 뽑기입니다.
약간 직소퍼즐 풀때를 생각하시면 되는데요, 우선 직소퍼즐풀때 테두리를 맞추고나서 한눈에 알아보기 쉬운 부분이 있는 부분을
찾아서 맞추죠. 막 숲이나, 물결이나 일관되게 전체적으로 있는거 나오면 맞추기 힘들구요.
이 특징점 뽑는것도 그렇게 생각하시면 되는데요, 그림과 같이 A, B, C가 있다면 어떤게 더 찾기 쉬울까 생각해보면
대충봐도 C가 더 쉽겠죠. 삼각뿔모양 세군데중 하나일거 같은데, 그 다음은 B일건데, B 정도도 쉽지 않죠. 그래도
가로로 희미하게 난 줄 때문에 벽면 중 어디인것 같고, A의 경우 하늘 어디일거 같은데 잘 모르겠죠.
결국 이 장에서 설명하고 싶은건 이건데요, 어떤 부분이 다른 부분과 비교해서 구분이 되는 부분이 있으면
그 부분을 특징점이다 라고 생각해도 된다는 거죠.

이 생각에 기반해서 특징점이란걸 70년대 말에 영상처리 부분에서 사용한게 모라벡이란 사람인데요,
갑자기 수식이 나와서 당황스러우시겠지만, 결국 이 수식에서 얘기하고 싶은건 이거에요,
우리 중고등학교때 기억을 되돌려보면 벡터라던지 유클리디안디스턴스라던지 두 부분의 차이를 비교할 때
제곱을 해서 비교를 했잖아요, 그렇게 제곱해서 비교하는걸 이미지 평면에서도 하겠다는거에요.
평면에 u, v, 라는 점이 있을때 주변 점들과의 차이가 얼마나 나냐 라는걸 보는 식이죠.

근데 이 모라백이란 사람이 만든건 여러가지부분에서 단순한 부분이 있어서 실제환경에서 쓰기에는 한계가 있었는데요,
비교를 할때 좌우상하의점과 discrete 하게 비교를 해서 계산을 하기 때문에 로테이션이 있는 경우에는  문제가 생기는데
이 문제를 해결한게 Harris corner라는 것입니다. 단순히 어떤점을 계산에 포함하느냐 안하느냐를 1,또는 0으로 계산했는데
그 부분을 Normal distribution 을 통해서 계산을 하게되고, 회전된것을 반영하기 위해 미분 개념을 도입하게 됩니다.
그리고 이게 또 2004년에 SIFT라는것으로 발전하게 되는데요, 기존의 Harris corner는 스케일변화에 민감하지 못한 경향이 있는데
이 부분을 이미지피라미드 형태로 해결한게 SIFT라는 애입니다.
장황하게 설명드렸는데, 지금까지 Feature extraction 이란것에서 기억하셔야할건 주변 점들과 비교를 해서
구분이 될만한 점을 골라내야한다는 거였습니다.
 
이렇게 뽑은 특징점은 Descriptor 라는 것을 만들어야하는데요, 이 Descriptor라는건 특징점이 주변과 비교를 했을 때
어떻다 라는 정보입니다. 주변과 비교했을때 어떻다 하는 정보가 있어야 다른장에서 특징점이 추출되었을때 같은 점인지
확인을 할 수 있겠죠. 이 정보들은 특징점 추출 방법에따라 정보가 다른데, 지금 보시는건 앞에서 설명한 SIFT의 Descriptor입니다.
특징점 주변을 16*16 픽셀을 대상으로 각 픽셀의 image gradient를 측정하고 해당 gradient를 4*4 픽셀 기준으로 8방향의
벡터값을 기준으로 뽑습니다. 이렇게 되면 128차원의 벡터가 생성되고 이 벡터를 비교하는 것으로 같은 점인지 아닌지를
판별한다고 합니다.
장황하게 설명드렸는데, 이부분에서 이해하실건 어떤 특징점을 뽑고 그 특징점이 주변점과 어떤 관계를 가진다 라는것을
Descriptor라는것으로 만들어서 비교를 한다는 부분만 이해하시면 될 것 같습니다.

그래서 결국은 SIFT란걸 쓰면 되는거네... 라고 하고 있었는데 알아보다보니
이 SIFT는 특허가 있어서 연구목적으로는 쓸수 있는데 상업적인 용도로는 사용할 수 없다고 하더라구요,
그래서 찾다보니 BRISK라는 더 발전된 방식이 있어서 이 방식을 사용해보자 해서 개발을 진행했습니다.

앞에서 장황하게 특징점이 어떻고 Descriptor가 뭐고 설명을 드렸는데 실제 이 모든것을 OpenCV에서
이미 제공을 하고 있어서 지금 보시는것과 같이 간단하게 몇 줄만 가지고도 특징점을 추출할 수 있는데요,
보시면 imageMat 라는건 이미지의 각 픽셀정보고, keyPoints라는건 이제 특징점의 위치를 x,y로 받을 스트럭쳐고
descriptor는 앞에서 말씀드린 디스크립터구요.
파라메터값을 설정후에, 디텍터라는것을 만들어서 찾아서 계산해줘. 하는 함수를 한번 호출하면
결과값이 나옵니다. 참 쉽죠...

예 보시는것은 이렇게 추출된 특징점들을 이미지에 표시해본것입니다. 회색인 이유는 BRISK가
gray 영상을 가지고 처리하는 애라서 그런것이고, 보시면 이제 특징점이란게 어떤것이구나 아시겠죠.


다음으로 해야하는 작업은 Feature Matching 작업입니다.
어떤점이 다음 이미지들에서 어디에 위치하는 것을 파악하는 작업인데요,
지금 보시면 아시겠지만 어떤점들은 일관되게 잡히고, 어떤점들은 나타났다가 사라졌다가 하는 것을 볼수 있죠?
결국 여기서 하고 싶은건 이제 일관되게 나오는 애를 찾고 그 점이 얼마나 이동했는지를 확인하는 일입니다.

이부분에선 앞에서 말씀드린 Descriptor라던지, 단순 거리를 재는 Euclidean Distance, 여기에 확률부분을 적용한
Mahalanobis distance, 그리고 빠르게 찾기위해 Kd-tree나 Hashing 이라는 것들을 이해해야하는데요,

마찬가지로 이 부분에 대한 이해가 크게 없어도, 보시는것과 같이 opencv에서 제공하는 함수로
간단하게 구할 수 있구요, 보시는 코드는 k-nearest neighbor 라는 알고리즘을 이용해서 매칭된 점을 뽑는 함수입니다.

이렇게 매칭을 하고나면 왼쪽같이 뽑혔던 점들이 오른쪽과 같이 일관되게 나오는 애들만 남길 수 있게 되는거죠.
자세히 보시면 약간의 에러가 있긴한데 별로 크리티컬해보이진 않죠.

이제 마지막 단계가 남았는데요. 이제 각 점들이 얼마나 이동했는지를 찾아야하는데,
기억을 잘 해보시면 선형대수학시간에 메트릭스에 점을 곱하면 막 이동하던게 생각나실거에요.
3차원에서 이동을 하게하는 메트릭스값을 결국 찾고 싶은거고, 
