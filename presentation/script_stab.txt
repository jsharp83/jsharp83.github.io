안녕하세요 폴라개발 전은철입니다.
간단하게 제 소개를 하면 저는 올해 3월부터 네이버에 들어와서 폴라 개발을 시작했고,
원래 영상처리를 하던 사람도 아니고, 폴라앱이 영상처리와 많이 관련된 앱도 아닌데,
오늘 발표는 연속촬영된 이미지들을 보정하는 영상처리 관련된 내용을 하게 되었는데요.
그러다보니 영상처리 관련해서 깊이 있는 내용을 얘기할 건 아니구요,
그냥 영상처리쪽이 생각보다는 쉽구나. 잘 모르는데 어떤 프로젝트 할 때
영상처리쪽 기술이 필요하면 내가 간단하게 짜봐도 되겠구나 하는 생각이 들었으면
좋겠다고 생각하면서 만든 발표자료입니다.
아무튼 발표내용은 루픽 손떨림방지 기능개발에 대한 내용입니다.

우선 루픽이라는 기능에 대해서 설명을 드려야하는데,
요즘 움짤이 대세다보니 움짤을 만들어줄 수 있는 기능을 넣어보자 해서 폴라에 들어간 기능이구요,
0.25초 간격으로 7장의 사진을 찍은 후에 보시는것과 같이 animated GIF로 만들어주는 기능입니다.

예.. 익숙한 곳이죠... 근데 루픽 개발이 끝나고 신나게 찍고 다니는데
약간 불만족스러운 부분이 있더라구요...
예 떨림이 심한건데요.. 아무래도 사람이 들고 찍고, 0.25초 간격으로 사진을 찍다보니
흔들림이 심할 수 밖에 없고... 특히 저는 저희팀에서 사진을 못찍는 사람 중 하난데,
원래 한장을 찍어도 흔들리는 사람이에요.. 그러다보니 손떨림을 꼭 넣어야겠다 싶었구요.

그래서 설떨림 방지 기능을 알아보고 있었는데, 많은 솔루션들이 있어요.
렌즈시프트라고 해서 자이로로 기울기를 받고 기울기의 반대로 렌즈를 움직여주는 방식도 있고,
왼쪽 그림처럼 센서를 움직여주는 방식도 있습니다.
이렇게 하드웨어적인 방식들도 있고, 촬영된 동영상에서 떨림을 제거하는
소프트웨어적인 방식도 있는데, 소프트웨어적인 방식은 패키지로 제공되는 솔루션들이 좀 있는거 같긴 한데,
제가 잘 못찾은걸수도 있는데 공개된 소스가 거의 없더라구요.
그래서 개발에 들어갔습니다.

우선 개발하기전에 제약사항을 파악을 했는데,
몇가지 요소가 있었어요
알고리즘이 실시간으로 처리되어야 하는가, 장수가 굉장히 많아서 전체를 가지고 처리하기 힘들고, 한장한장씩 이터레이티브하게
처리를해야하는가. 떨림정도가 어느정도인가..개발기간은 얼마나 있는가 정도였는데요.

우선 이 알고리즘이 빠르면 좋긴한데 실시간으로 처리될 필요까지는 없었어요. 이미지를 촬영한 후에 필터를 입히고, GIF로 만드는
과정에서 이미 프로그레스바를 띄워주고 있는 상황이라 사용자가 참을 수 있을정도의 속도면 괜찮다고 생각했습니다.

이터레이티브한지도, 장수가 7장밖에 안되기때문에, 동영상처럼 1초에 30장씩 나오는 이미지들을 처리를 하는 것 보다 훨씬 쉬운 상황이었구요.

떨림은 애초에 이 루픽이라는건 사용자가 손을 떨지 않는다는 생각을 하고 촬영을 하는 것이고,
떨림이 심할때는 그냥 손떨림을 제외시키면 되는 상황이라 크게 고려하지 않아도 되었구요.

개발기간은 이건 스펙에 있었던것도 아니고, 그냥 그런 기능도 있었으면 좋겠다 하는 기능이기 때문에
아무도 신경을 안쓰고 있었습니다. 그냥 개발해보고 안되면 아 시간이 좀 많이 걸리겠네요. 하면 되는 상황이었죠.

이렇게 제약사항을 파악하고 나니 굳이 개발을 못할 이유가 없었어요.
그래서 최대한 간단하게 구현해보자. 기존의 일을 하면서 가끔 시간이 날때 해보고 안되면 말지 하는 마음으로 시작했는데요.
지금 발표자료 보시면 최대한 간단하게 구현해보자는 글자가 다른 글자들에 비해 급 작아졌죠.
시작의 마음은 이랬는데, 실제 개발을 끝내고나서 생각은 간단하게 구현하는건 아는것이 많을때나 가능한거고,
아는게 없는 상황에서는 시행착오를 하고, 복잡하게 구현이 될 수 밖에 없다는걸 다시 한번더 깨닳은...

아무튼 여기까지는 서론이었고, 이제 실제 알고리즘의 프로세스를 구상하는 단계입니다.

문제인 이미지가 떨린다. 이 문제 자체는 아까 하드웨어적인방법에서 랜즈가 움직이던 센서가 움직이던
얼마나 움직였는지를 파악해서 되돌려주면 되지 않는가 하는거죠.
그럼 얼마나 이동했는지는 어떻게 아는가? 그건 뭐 전장에서 나왔던 부분이 다음장에서 어디에 위치하는가를 보면
알수 있지 않을까 싶었습니다. 간단하죠.

정리를 해보면
우선 각 이미지에서 특징점이라는것을 뽑습니다.
일단 특징점이라는건 이미지들에서 이 부분은 주변 점들과 비교했을 때 구분이 되는 점입니다.
다음으로 이전이미지와 비교를해서 이전 이미지와 현재 이미지에 공통적으로 나타난 특징점을 뽑아냅니다.
특징점이란건 안뽑히는거에서는 몇십개, 진짜 단색의 이미지에서는 아예 안뽑히고, 많이 뽑히는 곳에서는 이삼천개씩 뽑히기 때문에
뽑힌 특징점에서 같은 특징점이란걸 찾는 과정이 필요합니다.
이 과정 이후에 같은 특징점을 찾았으면, 이미지 평면상에서 좌 우로 얼마나 움직였으니, 실제 촬영했던 카메라 자체는
3차원에서 어떻게 움직였는지를 예측해내는 과정이 있게 됩니다.
단순히 생각해도 카메라가 줌인 줌아웃 됬다고 하면 왼쪽에 있던 점들은 오른쪽으로 움직였을 것이고,
오른쪽에 있던 점들은 왼쪽으로 움직이게 되겠죠. 어떤 점들이 오른쪽으로 얼마만큼 이동했다고 단순히 카메라가 오른쪽으로 이동했다고
생각할 수는 없는겁니다. 그래서 뽑혀진 점들에서 카메라의 이동파라메터를 찾는 추정과정이 있게됩니다.

다음으로 각 세부파트에 대해서 설명하기 전에 주의사항으로,
지금부터 설명하는 프로세스는 최선이 아닐 수 있습니다.
더 좋은 방법이 있는데 제가 찾지 못했을 수도 있고, 시간이 지나게 되면 당연히 더 좋은 방법이 나올거구요,
이 방법은 제가 단순히 빨리 생각난걸 구현해보고, 구현해봤더니 이런 케이스에선 나쁘지 않은 성능을 보여주더라 하는거고
문제 해결에 집중하기 보다는 저렇게 푸네... 저정도는 나도 하겠는데 에 집중해주세요


특징점 뽑기입니다.
약간 직소퍼즐 풀때를 생각하시면 되는데요, 우선 직소퍼즐풀때 테두리를 맞추고나서 한눈에 알아보기 쉬운 부분이 있는 부분을
찾아서 맞추죠. 막 숲이나, 물결이나 일관되게 전체적으로 있는거 나오면 맞추기 힘들구요.
이 특징점 뽑는것도 그렇게 생각하시면 되는데요, 그림과 같이 A, B, C가 있다면 어떤게 더 찾기 쉬울까 생각해보면
대충봐도 C가 더 쉽겠죠. 삼각뿔모양 세군데중 하나일거 같은데, 그 다음은 B일건데, B 정도도 쉽지 않죠. 그래도
가로로 희미하게 난 줄 때문에 벽면 중 어디인것 같고, A의 경우 하늘 어디일거 같은데 잘 모르겠죠.
결국 이 장에서 설명하고 싶은건 이건데요, 어떤 부분이 다른 부분과 비교해서 구분이 되는 부분이 있으면
그 부분을 특징점이다 라고 생각해도 된다는 거죠.

이 생각에 기반해서 특징점이란걸 70년대 말에 영상처리 부분에서 사용한게 모라벡이란 사람인데요,
갑자기 수식이 나와서 당황스러우시겠지만, 결국 이 수식에서 얘기하고 싶은건 이거에요,
우리 중고등학교때 기억을 되돌려보면 벡터라던지 유클리디안디스턴스라던지 두 부분의 차이를 비교할 때
제곱을 해서 비교를 했잖아요, 그렇게 제곱해서 비교하는걸 이미지 평면에서도 하겠다는거에요.
평면에 u, v, 라는 점이 있을때 주변 점들과의 차이가 얼마나 나냐 라는걸 보는 식이죠.

근데 이 모라백이란 사람이 만든건 여러가지부분에서 단순한 부분이 있어서 실제환경에서 쓰기에는 한계가 있었는데요,
비교를 할때 좌우상하의점과 discrete 하게 비교를 해서 계산을 하기 때문에 로테이션이 있는 경우에는  문제가 생기는데
이 문제를 해결한게 Harris corner라는 것입니다. 단순히 어떤점을 계산에 포함하느냐 안하느냐를 1,또는 0으로 계산했는데
그 부분을 Normal distribution 을 통해서 계산을 하게되고, 회전된것을 반영하기 위해 미분 개념을 도입하게 됩니다.
그리고 이게 또 2004년에 SIFT라는것으로 발전하게 되는데요, 기존의 Harris corner는 스케일변화에 민감하지 못한 경향이 있는데
이 부분을 이미지피라미드 형태로 해결한게 SIFT라는 애입니다.
장황하게 설명드렸는데, 지금까지 Feature extraction 이란것에서 기억하셔야할건 주변 점들과 비교를 해서
구분이 될만한 점을 골라내야한다는 거였습니다.
 
이렇게 뽑은 특징점은 Descriptor 라는 것을 만들어야하는데요, 이 Descriptor라는건 특징점이 주변과 비교를 했을 때
어떻다 라는 정보입니다. 주변과 비교했을때 어떻다 하는 정보가 있어야 다른장에서 특징점이 추출되었을때 같은 점인지
확인을 할 수 있겠죠. 이 정보들은 특징점 추출 방법에따라 정보가 다른데, 지금 보시는건 앞에서 설명한 SIFT의 Descriptor입니다.
특징점 주변을 16*16 픽셀을 대상으로 각 픽셀의 image gradient를 측정하고 해당 gradient를 4*4 픽셀 기준으로 8방향의
벡터값을 기준으로 뽑습니다. 이렇게 되면 128차원의 벡터가 생성되고 이 벡터를 비교하는 것으로 같은 점인지 아닌지를
판별한다고 합니다.
장황하게 설명드렸는데, 이부분에서 이해하실건 어떤 특징점을 뽑고 그 특징점이 주변점과 어떤 관계를 가진다 라는것을
Descriptor라는것으로 만들어서 비교를 한다는 부분만 이해하시면 될 것 같습니다.

그래서 결국은 SIFT란걸 쓰면 되는거네... 라고 하고 있었는데 알아보다보니
이 SIFT는 특허가 있어서 연구목적으로는 쓸수 있는데 상업적인 용도로는 사용할 수 없다고 하더라구요,
그래서 찾다보니 BRISK라는 더 발전된 방식이 있어서 이 방식을 사용해보자 해서 개발을 진행했습니다.

앞에서 장황하게 특징점이 어떻고 Descriptor가 뭐고 설명을 드렸는데 실제 이 모든것을 OpenCV에서
이미 제공을 하고 있어서 지금 보시는것과 같이 간단하게 몇 줄만 가지고도 특징점을 추출할 수 있는데요,
보시면 imageMat 라는건 이미지의 각 픽셀정보고, keyPoints라는건 이제 특징점의 위치를 x,y로 받을 스트럭쳐고
descriptor는 앞에서 말씀드린 디스크립터구요.
파라메터값을 설정후에, 디텍터라는것을 만들어서 찾아서 계산해줘. 하는 함수를 한번 호출하면
결과값이 나옵니다. 참 쉽죠...

예 보시는것은 이렇게 추출된 특징점들을 이미지에 표시해본것입니다. 회색인 이유는 BRISK가
gray 영상을 가지고 처리하는 애라서 그런것이고, 보시면 이제 특징점이란게 어떤것이구나 아시겠죠.


다음으로 해야하는 작업은 Feature Matching 작업입니다.
어떤점이 다음 이미지들에서 어디에 위치하는 것을 파악하는 작업인데요,
지금 보시면 아시겠지만 어떤점들은 일관되게 잡히고, 어떤점들은 나타났다가 사라졌다가 하는 것을 볼수 있죠?
결국 여기서 하고 싶은건 이제 일관되게 나오는 애를 찾고 그 점이 얼마나 이동했는지를 확인하는 일입니다.

이부분에선 앞에서 말씀드린 Descriptor라던지, 단순 거리를 재는 Euclidean Distance, 여기에 확률부분을 적용한
Mahalanobis distance, 그리고 빠르게 찾기위해 Kd-tree나 Hashing 이라는 것들을 이해해야하는데요,

마찬가지로 이 부분에 대한 이해가 크게 없어도, 보시는것과 같이 opencv에서 제공하는 함수로
간단하게 구할 수 있구요, 보시는 코드는 k-nearest neighbor 라는 알고리즘을 이용해서 매칭된 점을 뽑는 함수입니다.

이렇게 매칭을 하고나면 왼쪽같이 뽑혔던 점들이 오른쪽과 같이 일관되게 나오는 애들만 남길 수 있게 되는거죠.
자세히 보시면 약간의 에러가 있긴한데 별로 크리티컬해보이진 않죠.

이제 마지막 단계가 남았는데요. 이제 각 점들이 얼마나 이동했는지를 찾아야하는데,
기억을 잘 해보시면 선형대수학시간에 메트릭스에 점을 곱하면 막 이동하던게 생각나실거에요.
3차원에서 이동을 하게하는 메트릭스값을 결국 찾고 싶은거고, 또한 점들을 뽑는것에서 에러가 포함되었기때문에
그 메트릭스가 하나로 딱 떨어지진 않을거라는거죠.
그래서 에러를 최소한으로 하는 메트릭스를 찾아야 하는데 그 과정에서 Ransac 이라는 알고리즘이 사용되게 됩니다.

손떨림보정 부분을 개발하면서 유일하게 제가 사전지식이 있던 부분이 이부분인데요,
예전에 비슷한 작업을 한적이 있어서 아 이건 금방짜지 하고 아무 생각없이 개발에 들어갔습니다.
이전장에 있던 점들을 aList 에 담고, 현재장에 있는 점들을 bList에 담아서 wiki에서 수식찾아서
구현을 해서 제대로 나오나 봐볼까 싶던 순간에...
보시면 아시겠지만 이부분만 소스를 긁지 못하도록 이미지로 처리되어있죠...
OpenCV에서 다음과 같이 해당 메트릭스를 찾아주는 함수가 존재하고
당연히 제가 직접 짠 소스보다 검증이 잘 되있겠죠..
미련없이 짰던 소스를 주석처리하고 해당 함수를 사용하였습니다.
여기서의 교훈은 뭔가 조금 더 안다고 바로 개발하지말고 구현하기 전에 꼼꼼하게 잘 찾아보자...
제가 생각하기에는 현대를 사는 개발자의 필수 덕목인거 같아요.
구현하기전에 잘 찾는다.

마지막으로 이렇게 찾은 메트릭스를 가지고 이미지를 보정하는 작업입니다.
이 부분도 역시 간단하게 한줄로 해결이 됬습니다.

여기까지 개발했을 때 결과입니다.
왼쪽은 0.25초 간격으로 찍었던 Loopic image 들이고 이걸가지고 보정을 한게 오른쪽 이미지들입니다.
약간의 떨림이 남아있긴한데 왼쪽과 비교하면 확실히 덜떨리는 느낌이죠.
여기까지 개발했을때 저는 이제 끝난줄 알았어요.
원래 이건 일정에도 없고 본업도 아니라 그냥 시간 남는시간에 했는데, 이정도면 뭐 괜찮네.
이러면서 팀 사람들한테 막 메신저로 자랑했죠. 된거같다고.

아까 말했던부분까지 개발을 하고 기획하시는 분께 얘기했죠. 아 구현 된거같다.
그래서 보자. 하고 앱을 가져갔는데
이분이 제가 예상했던거랑은 전혀 다른 행동을 하시더라구요.
의자에 앚아있으시더니 갑자기 수영할때처럼 다리를 막 파닥파닥 물장구치면서 그걸 루픽으로 찍으시는거에요.
순간 멘붕에 빠졌죠.. 이거 뭐지...
당연히 움직이는 물체가 있으면 알고리즘이 동작을 할 수 없죠.
이건 특징점이란걸 잘 잡으면 잘 잡을수록 문제가 되는데, 서로 다른 속도로 움직이는 것들을
하나의 메트릭스로 뽑게 되면 이상한 값이 나올수 밖에 없죠.

고민을 했습니다. 어떻게 하면 해결할수 있을까.
예전 학교다닐때 줏어들은 지식으로 점들을 classification 해야하나.
여러 생각들이 있었는데 우선 가장 간단하게 해보자 하고 생각이 정리됬어요.
간단히 생각해보면 이렇죠. 움직임이 큰 애는 빼. 그리고 전프레임에서 일관되게 나오는애만 남겨.

왼쪽이 원본 이미지고, 오른쪽이 일단 특징점들을 추출한 상황입니다.
보시면 눈으로봐도 움직이고 있는부분에서 매칭된 특징점들을 이동이 큰것을 확인할 수 있죠

예 이렇게 일정 쓰레시홀드값 이상으로 움직이는 것들을 제외시키고 공통으로 나오는 애들만 매칭한 경우가 오른쪽인데요
확실히 왼쪽과 비교해서 보시면 움직이는 부분에는 점이 매칭되지 않을걸 확인하실 수 있죠.

그래서 결과화면입니다. 완벽하게 떨림이 사라졌다고 하긴 어려워도 왼쪽에 비해서는 확실히
움직임이 줄어들었죠.

여기까지 하고나서 이제 기획자분께 가져갔더니
아 된거 같다고. 이정도면 실제 서비스에서 쓸수 있겠다해서

그래서 실제 버전에서 쓰려고 작업을 하다보니까
iphone5에서 전체 알고리즘을 돌리는데 18초가 걸리는건거에요.
이게 개발하면서 시뮬레이터랑 6에서 개발했는데
속도가 좀 느린데 싶긴했는데 5에서는 18초나 걸릴지는 예상을 못했죠.
아... 이제 이거 다 만들었는데 모바일이라 속도가 안되나... 생각하면서
일단 github에 정리를했습니다.
대략 18초가 걸리는데 각각 시간 걸리는 부분을 봤더니
크게 두가지 부분이 있는데 Feature를 뽑는데 11초가 걸리고 매칭후에 inliner만 남기는데 5.5초가 걸리는거에요.
일단 이미지 한장에서 피쳐들을 뽑는데, 한장당 많이 걸릴때는 2초까지도 걸리는건거죠.
그런데 이 부분은 제가 코드에서 시간을 줄일만한여지는 없었어요.
아까도 코드를 보여드렸지만 이 부분은 두줄로 해결했거든요.

그럼 어떻하냐.. 피쳐뽑는부분 알고리즘을 BRISK라는것을 썼는데, 다른걸로 바꿔보자.
해서 찾아봤는데 피쳐뽑는부분에도 굉장히 많은 알고리즘이 있어요.
지금보시는건 FAST라는 알고리즘인데요 간단하게 설명드리면
p라는것이 특징점이 되려면 반지름이 3픽셀인 주변점들 16개를 봐요.
이 16개와 p라는 점의 색차가 일정쓰레시홀드를 넘느냐 안넘느냐. 그 넘는것들이 몇개나 되냐.
몇개 이상정도가 되면 이 점을 특징점이라고 생각해도 되지 않겠냐.
하는건거에요. 딱보기에도 아까 말씀드렸던 SIFT보다 훨씬 계산이 간단할거 같은 느낌이나죠.
그래서 피쳐를 뽑는데 이 알고리즘으로 딱 바꿨더니...
피쳐 뽑는부분에서만 11초걸리던게 0.5초

아 이정도면 많이 줄였네. 쓸만해졌다.
근데 아까 5.5초 걸린다는 부분도 이해가 되지 않았어요.
그 부분은 두 점을 단순 비교하는 건데 갯수가 많다고 해도 그정도까지 걸릴 일은 아니었거든요.
분홍색이 이전코드고 초록색이 현재 코드인데요,
위처럼 하면 벡터를 계속 복사하는 형태가되죠.
아무튼 이 부분을 수정했더니 5.5초 걸리던부분이 0.5초로 줄어들게 되었습니다.
그래서 속도향상까지해서 현재 폴라에 손떨림방지부분이  적용되어있습니다

예 이제 발표의 마지막 부분인데요.
아이폰 개발을 하고나서 안드로이드쪽도 손떨림 부분을 개발을 했고,
처음에는 편하게 가려고 JavaCV라고 OpenCV를 Java버젼으로 컨버팅한것을 썼는데
소스관리하기도 힘들고 속도도 빠른것 같지 않아서 NDK 쪽을 썼고
아이폰은 4~5메가, 안드로이드는 8~9 메가 정도 앱 용량이 증가했습니다.

이 밖에도 현재 알고리즘을 개선할 여지가 몇개 있어요, 속도를 더 빠르게 하기위해 GPU 연산을 하는 부분을 추가한다거나
특징점을 화면의 일부 부분에서 뽑아서 계산이 잘못되서 울렁거리게 나오는 경우가 있는것 같은데
그런부분도 제거를 하고, 흔들림자체를 예측하는부분의 정확도를 향상시키기 위해 칼만필터같은것도 적용하면 좋겠다 생각했는데
아직 추가적으로 구현하지는 못하고 있는 상태구요.

이렇게 개발한것은 소스를 정리해서 현재 Github에 공개해둔 상태입니다.
아래 주소에 가시면 받으실수 있습니다. 감사합니다.
