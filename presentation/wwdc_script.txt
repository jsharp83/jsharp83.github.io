안녕하세요, 동영상셀의 전은철입니다. 저는 WWDC2016에서 발표된 내용중에 미디어와 관련된 내용을 좀 정리해봤구요, 우선 WWDC가 어떻게 진행되는지, 그리고 세션들이 어떻게 구성되어있는지를 간단히 설명드린 후에, 미디어관련해서 설명드리겠습니다.

WWDC는 샌프란시스코 시내 안에 두군데 장소에서 진행이되는데요, 왼쪽이 Bill Graham civic Auditorium 이라고 해서 WWDC에 참가하는 전체인원이 들어갈수있는 큰 공연장같은 느낌의 장소고, 오른쪽이 Moscone West라고하는 건물인데 각 세부세션들과 Lab이 진해되는 장소로 두 건물 사이는 걸어서 한 20~30분 정도 거리에 있습니다.

우선은 일요일날 도착해서 Civic 센터에서 본인이름으로 된 출입증과 기념점퍼를 받았습니다.
월요일에는 Civic센터에서 키노트와 플랫폼 스테이트 오브더 유니온 세션이 있는데 키노트가 약간 일반인 대상으로 기술의 변화를 설명하는 세션이라고 하면 플랫폼 스테이트 오브더 유니온은 그런 기술변화들이 개발자 입장에서 어떤 SDK가 어떻게 변했다를 설명해주는 세션입니다. 오른쪽 이미지는 세션후에 풀밭에 앉아서 점심용 센드위치 먹고있는 장면입니다.
화요일부터 금요일까지는 모스콘 웨스트라는 곳에서 세션과 랩을 진행을 합니다. 건물 1층에서 오른쪽과 같이 Lab이 있어서 질문이 있는 사람들은 줄서서 애플 개발자들에게 질문을 할 수 있게 준비되어있구요, 2층,3층에서는 세션이 동시에 4개정도 열렸습니다. 오른쪽 사진은 페더리기라고 개발총괄하시는 분인데, 건물을 돌아다니다가 보면 사진도 찍을수 있었습니다.
세션들이 다 끝나고난 금요일 오후에는 Bash라고 해서 행사를 마무리하는 파티까지하고 나면, WWDC행사가 끝이나게 됩니다.

아시겠지만 WWDC영상들은 디벨로퍼사이트를 통해 공개되어있고, 위와같은 카테고리로 정리되어있는데요,
200번대 세션인 App framework세션에서는 iMessage라던가 sirikit 과 관련된 내용이있었고
디자인 세션에는 지금 이 다섯개 정도의 세션이 있구요.
Developer Tool 세션에는 Swift API Design GuideLine같은 세션이 있었습니다.
Distribution Session에는 Subscription관련해서 기능이 추가된 내용이라던지 앱스토어 검색부분에 광고가 붙게되는데 그것과 관련된 내용이 있었습니다.
피쳐드에는 키노트랑 플랫폼 스테이트오브더 유니온 세션이 있었고
시스템 프레임웤 세션에는 노티피케이션관련해서 이번에 추가된 내용들이 있는데 그 부분과 관련된 부분, 홈킷이라던지 뉴럴넷 관련된 설명이 있는 세션이 있었습니다.

미디어와 관련된 세션은 500번대 세션으로 총 11개의 세션이 있었는데요, 우선 Advances in iOS PhotoGraphy라는 애가 501 번 세션인데 이미지캡쳐관련해서 AVCapturePhotoOutput이란 API를 설명하는 세션이었고, Live Photo Editing은 CIImage를 이용해서 Live Photo와, Raw format 이미지를 filtering 하는 내용입니다. What’s new in HTTP Live Streaming은 HLS관련해서 추가된 내용들을 설명하는 세션이었고 Advances in AVFoundation Playback은 AVPlayer의 재생상태관련해서 추가된 내용을 설명하는 세션이었습니다. Graphics and Game세션은 총 12개 였는데요, 그중 Go live with replayKit은 작년 ios9에 추가된 기능인 화면녹화기능이 이제 Live Stream 기능까지된다는 내용의 세션이었고, Metal part2는 Metal Performance shader를 딥러닝에 활용하는 내용등이 있었습니다.

세부내용을 좀 더 살펴보면, 우선 새로운 AVCaptureOutput인 AVCapturePhotoOutput이라는 애가 소개 되었는데, 얘가하는 역할은 ios10부터 Live Photo를 캡쳐하고 편집할수 있게되었고, RAW포멧 이미지를 가져오고 처리할수 있게 되었으며, 캡쳐단에서 Preview Image를 가져올수있고 wide color포멧을 지원해주게 되었습니다.

기존에 이미지캡쳐는 왼쪽과 같은 프로세스였는데요, AVCaptureSession이 있고, AVCaptureDevice로 부터 데이터를 받아서 AVCaptureStillImageOutput이라는 애를 써서 캡쳐를 했었는데요, 얘가 deprecated되고 오른쪽과 같은 AVCapturePhotoOutput을 사용하게 되었습니다. 사용방법은 우선 AVCapturePhotoOutput이라는 애를 만들고, 기존에는 설정부분을 Dictionary형태로 넘겼었는데, 내용이 많아져서 AVCapturePhotoSetting이라는 객체를 만들었습니다. 이 객체에 설정내용을 담아서 capturePhoto부분에 넘기면 이제 이벤트가 발생할때마다 delegate를 받게 됩니다.

소스코드를 봐보면 우선 하이레죨루션이미지를 캡쳐하고 싶다 하면, AVCapturePhotoSettings를 만들어주고, 세팅의 isHighResolutionPhotoEnabled를 true로 하고 AVCapturePhotoOutput의 capturePhoto부분으로 세팅과 delegate를 넘겨주면 됩니다.

기존에 사용하던 AVCaptureStillImageOutput과 AVCaptureDevice밑에 있던 플래시관련된 변수들이 deprecated되고 AVCapturePhotoOutput쪽으로 옮겨졌습니다.

다음으로는 Live Photo관련 기능인데요, 기존에는 카메라에서 촬영만되고, Viewer형태로 보는 부분만 있었는데, 이부분을 촬영하고 편집하는 기능이 추가되었습니다.

라이브포토 데이터는 이미지랑 촬영버튼을 누르기전 1.5초, 누른후의 1.5초 해서 총 3초정도의 동영상으로 구성이 되는데요, ios10부터는 video stabilization기능과 캡쳐시에 음악이 끊기던 부분이 수정되었다고합니다.

라이브포토를 저장할땐 지금보시는것과같이 우선 AVCapturePhotoSetting을 만들어주고요, setting의 livePhotoMovieFileURL 저장할 파일URL을 지정해주고, 동영상 메타데이터 설정하는 부분이고 capturePhoto부분으로 넘겨주면 됩니다. 동영상은 앞에서 세팅한 URL에 저장되고, 이제 이미지를 저장해야하는데 delegate로 지금보시는것과같이 PhotoSampleBuffer가 넘어오면 이미지를 저장하면 됩니다.

다음으로는 LivePhoto를 보여주는 부분인데요, 이미지와 동영상을 PHImageManager가 PHLivePhoto라는 객체로 만들어주고 PHLivePhotoView가 보여주는 형태입니다.

라이브포토를 편집하는 내용도 있는데 이 부분은 505세션을 확인하시면 됩니다.

다음으로 RAW이미지포멧을 캡쳐하는 부분인데요, 우선 CMOS Sensor는 두개의 Layer로 이루어지는데 Color Filter Array와 Sensor Array입니다. 빛이 Color Filter Array를 통과하면 해당색 한가지만 남게되고, Sensor Array는 그 한가지 색상을 센싱하는 구조인데요, 지금 보시면 초록색이 빩간색이랑 파란색에 비해 두배로 많은데, 그 이유는 사람눈이 녹색에 더 민감하기 때문이라고 합니다. 아무튼 이렇게 Sensing된 데이터 자체가 raw format형태이고, 기존에는 이것을 한번 가공한 jpeg 형태로 이미지가 캡쳐되었는데, 이제는 이 raw format 형태로도 데이터를 가져올수있게되었습니다. 
코드상에서 raw format 을 가져오는 부분은 지금보시는것처럼 AVCapturePhotoSetting을 만들어줄때 rawFormat으로 만들어줘 하면 되는데, Raw Format은 후면카메라일때만 동작을 한다고 합니다.
이미지를 받았을때 지금보시는것처럼 delegate가 불리고 DNG라는 Adobe에서 만든 Raw 파일 format이 있는데, 앞으로 이 확장자를 이용해서 아이폰쪽에서도 raw image를 다룬다고 합니다.

다음으로는 프리뷰이미지를 캡쳐하는건데요, 이전에는 큰 이미지를 가져와서 downscale해서 preview 이미지를 만들어줬었는데, 앞으로는 캡쳐단에서 downscale이미지를 얻을수 있다고 해요. 코드상에서는 지금보시는것같이 AVCapturePhotoSetting밑에 previewPhotoFormat이란 부분이 있어서 이 부분을 설정해주면 해당 크기의 프리뷰 이미지를 받을 수 있다고 합니다.

다음으로 설명드릴것은 Wide Color 부분인데요, iPad Pro 9.7부터 True Tone Display라는 기능이 제공되고 ios9.3 부터 Color management기능을 제공하고 있습니다.
기존에는 sRGB형태의 지금 보시는 정도의 칼라 영역을 표현해줬다고 하면 그것보다 초록색과 빨간색 부분을 더 표현하는 Display P3 Color Space를 쓰는 장비들이 있다는 얘긴데요, 캡쳐하는 부분에서는 크게 신경쓸 필요없이 AVCaptureSession에 automaticallyConfiguresCaptureDeviceForWideColor 라는 변수가 있는데 이걸 True로 세팅을 해주면 AVCaptureSession이 알아서 적합한 format과 color space 로 세팅을 해주게 됩니다.

다음으로는 Privacy 관련해서 변경된 부분이 있는데요, ios7부터 사진이나 카메라에 접근을 하려고 하면 왼쪽과 같은 팝업이 떴는데요, ios10부터는 조금더 강화가 되어서 오른쪽 보시는 부분과 같이 무슨목적으로 접근을 하는지를 명세해야한다고 합니다. 명세는 지금보시는것과같이 info.plist부분에 해주면 됩니다.

다음으로는 Replaykit 과 관련된 내용인데요, ios9 부터 지금보시는것과 같이 앱화면을 녹화할수 있는 SDK인 Replaykit이라는걸 제공하고 있었습니다. 보시는것과 같이 촬영을 시작하면 화면만 녹화할지 마이크도 같이 녹화할지를 물어보는 팝업이 뜨고, 녹화가 끝나면 해당 동영상을 볼수 있는 프리뷰뷰도 볼 수 있게 되어있는데요, 지금보시는것과 같이 코드도 굉장히 간단합니다. RPScreenRecorder라는 애의 인스턴스를 받아서 startRecodingWithMicrophone이란 함수를 호출하면 아까와같은 팝업이 뜨고요, stopRecodingWithHandler함수를 호출하면 preview를 띄울수 있는 viewcontroller가 넘어옵니다.

여기까지가 이제 ios9에서 제공해주는 기능이고 ios10부터는 실시간으로 broadcasting해주는 부분이 추가가 되었는데요, RPBroadcastActivityViewController라는걸 호출하면 팝업으로 Broadcast할수 있는 서비스를 고를 수 있는 팝업이 뜨고요, 고르고나면 세번째 함수와같이 딜리게이트 됩니다. 여기서 넘어온 broadcastController의startBroadcast해주면 브로드케스팅이 되고, 끝날때는 두번째함수처럼 finishBroadcast하게되면 끝나게 됩니다.
WWDC에서는 MOBCrush라는 서비스를 통해서 데모를 했었는데요, 두번째처럼 ActivityViewController를 띄우고, 세번째처럼 브로드케스트하는 정보를 넣고, 게임화면을 브로드케스팅하는 데모였습니다.

다음으로 Core image 관련된 변경사항인데요. 결국 필터링을 한다는건 이미지메트릭스에 어떤 kernel function을 곱한다는 건데요, CIFilter 쪽에서는 Fragment Shader 코드 비슷하게만들어진 Core Image Kernel Language라는 애가 있어요. 각각의 필터들은 이 CIKernel 이라는 애들이 있고, CIFiter는 이런 CIKernel들의 Chanin을 자동적으로 하나의 Concatenated된 프로그램으로 만들어서 적용을한다고 해요. 기존까지 180여개정도의 CIFilter가 이미 제공되고 있었고, ios10 부터 또 몇개의 필터가 추가 되었는데요, 지금 보시는것과 같이 CIHueSaturationValueGradient라고 해서 HSV color space 관련된 필터가 하나 추가 되었고, CINinePartStretched와 CINinePartTiled라고 해서 NinePatch와 관련된 필터인데 왼쪽과같은 이미지를 늘리려고 하면 아홉등분을 하고 그 가운데부분들을 스트레칭을 할거나. 타일형태로 배치해주는 필터입니다.
다음으로 CIEdgePreserveUpsampleFilter인데요, 6*7 픽셀의 어떤 필터이미지를 1024*768이미지에 씌워주려고하면 이제 이 이미즈를 확대를 해줘야하는데 그냥 확대를 한다면, 위에 이미지처럼 그냥 확대된 이미지가 나올껀데, 상황에 따라서는 이 가이드 영상의 Edge들을 살려줘야 할 때가 있어요 그랬을때 CIEdgePreserveUpsampleFilter를 이용하면 가이드된 이미지의 edge정보가 반영된 상태로 확대를 할 수 있게 됩니다.

성능적으로도 향상된 부분이 있는데요, GPU를 사용하는 Metal framework를 디폴트로 사용하게 바뀌었고, Core Image 단에서 처리한 CIImage를 결국 화면에 보여주려면 UIImage로 바꿔줘야하는데 이 부분 속도가 굉장히 빨라졌다고 합니다. Pixel Format도 기존에는 RGBA8 만 지원하다가 RGBAF, RGBAh 등도 지원하게 바뀌었다고 합니다.

다음으로 또 크게 추가된게 CIImageProcessor라는 애인데요, 기존에 Custom filter를 만들다보면 Core Image Kernel Laguage로만 필터부분을 짜는데는 한계가 있어요, 그래서 필터중간에 해당 프로세싱된 버퍼를 CPU단에서 처리를 하고 다시 필터를 입혀주고 해야하는데요, 이 부분이 CIImageProcessor라는 부분으로 빠져서 CPU나 Metal Code로 처리할수 있게 바뀌었습니다.
WWDC에서 CIImageProcessor를 사용하는 과정을 설명하기위해 예로든것이 Integral Image라는 것을 이용해서 Box blur를 입히는 과정을 예로 들었는데요. 

Integral Image 한국말로는 적분이미지인데요, 어려운 개념이 아니라 그냥 자기보다 앞에 나왔던 픽셀값들을 전부 더한 값이라는 개념이에요. 지금 오른쪽에 45가 어떻게 나왔냐.. 하면 그 45가 있는 왼쪽의 박스영역에 있는 모든 숫자를 더하면 45가 나옵니다.
어떤 개념인지 아시겠죠. 여기 10의 경우는 1+4+5를 하면 10이 나오죠. 17은 1+4+0+2+3+7을하면.. 17이 나오죠
아무튼 이런 Integral Image를 어디에 사용하는가 하면 지금보시는것과 같이 블러를 입힐때 사용을합니다. 블러라는건 크게 어려운건 없고, 주변값들과 다 합쳐준다음에 평균값을 내면 보시는것과 같은 블러효과를 볼 수 있습니다. 예를들어 지금 보시는 8이라는 지점의 픽셀값을 주변 값, 지금 파란색으로 처리된 부분을 다 더하고 평균낸 값이 블러된 픽셀의 값이 되는건거죠.

그럼 여기서 이 다 더하는 부분을 어떻게 계산을 할꺼냐… 보통을 걍 포문 두번 돌면서 다 더할꺼에요. 포 row는 1부터 3까지 for col은 1부터 3까지 해서 다 더해라 이렇게 짜겠죠. 이러면 n^2번의 read가 발생할겁니다. 조금더 빠르게 하자면 병렬처리를 하게 되면 2n 번read로 처리할 수 있을거구요. 이걸 Integral Image를 쓰면 박스크기와 상관없이 4번만에 처리를 할 수 있는데요. 현재 박스영역의 합을 알고싶다 하면 이부분값 66에서 이값 10, 13을 빼고, 이부분은 두번빠진 형태가 되니까 다시 한번 더해주면 같은 값이 나오게 된다는 건거죠. 아무튼 이건 중요하지 않습니다. 블러처리를 하는데 Integral Image라는게 중간에 있으면 좋다는건거죠.
본론으로 돌아와서 CIImageProcessor 를 이용해서 Integral Image를 만들어주는 부분인데요, input과 output으로 들어오는 CIImageProcessorInput output 객체는 CVPixelBuffer를 가지고 있고, CVPixelBuffer를 통해서 각 픽셀 정보에 접근 할 수 있습니다. 지금보시는것과 같이 for문을 돌면서 Integral Image 값을 계산할수 있고, 또는 지금 보시는것과같이 Metal Performance Shader를 이용해서 계산할수도 있습니다.

이부분코드는 이제 CIKernel 부분에서 Integral Image 데이터를 이용해서 픽셀값을 계산하는 부분인데요, 앞에서 말했던 방식대로 어퍼레프트 어퍼라이트 로워리프트 로워라이트 점을 뽑아서 그 점값으로 Blur 값을 구해주는 부분 코드입니다.

마지막으로 speech recognition 관련된 API인데요, 라이브음성이나, 녹음된 음성파일을 Speech Recognizer를 이용해서 텍스트 파일로 만들어주는 기능입니다.
기존에 키보드부분에 마이크로 표시되어있던 받아쓰기 기능으로 들어가있던 기능이 API로 빠져서 공개가 된 기능입니다.
이부분 코드는 굉장히 단순해요 SFSpeechRecognizer라는걸 만들어주고 이 객체에 해당파일을 던져주면 결과로 던져주는 애들중에 SFTranscription이라는 애가 있는데 얘의 formattedString을 보면 인식된 택스트를 알수있습니다.
더 자세한 내용은 SPEAKTOME라는 이름으로 코드가 공개되어있습니다.

정리를 해보면 IOS10에서는 Wide color를 지원하고 RAW Format 데이터의 이미지를 사용가능하구요,
Live Photo를 편집할수 있게되었습니다. Replaykit이 broadcasting을 지원하고, SpeachRecognition API가 공개되었습니다. 감사합니다.





안녕하세요. 네이버에 전은철입니다.
저는 WWDC 2016에서 발표되어서 ios10부터 적용되는 기술중에, Media와 관련된 내용에 대해서 설명드리겠습니다.

WWDC는 저번주에 끝났지만, 관련 동영상들은 지금도 웹페이지 들어가면 볼 수 있는데요, 잠깐 이 동영상들에 대해 설명드려보면, 일부는 실제 세션장에서 공개된것들이고, 일부는 Chalk talk 이라고 해서 추가로 설명을 더 하고 싶었던 부분을 녹화해둔 영상들이 있는데요, 전체 동영상들은 앱프레임워크 디자인 디벨롭퍼툴 디스트리뷰션 등으로 나눠져 있는데요, 우선 가장 중요한 동영상은 키노트랑 플랫폼스테이트 오브더 유니온인데요, 키노트가 약간은 일반인을 대상으로 앞으로의 변화에 대해서 설명해주는 것이라하면 Platforms state of the union은 개발자 입장에서 어떻게 바뀌고 앞으로 어떤 세션들이 있을거다를 설명해주는 세션입니다.

App frameworks 카테고리는 200번대 세션인데, iMessage, Sirikit 등에 대한 내용이 있었고,
Design 쪽은 이렇게 5개정도의 세션이 있었습니다.
Developer tools 세션은 400번대 세션인데 스위프트 API Guide 같은 세션도 인상깊었고, Debugging 관련된 세션도 좀 있었던거 같습니다.
Distribution쪽에는 5개 정도가 있었는데 Subscription 관련해서 기능들이 좀 늘어서 그런 설명들이 있었고, App store에 검색광고 붙는거 관련해서 세션이 있었습니다.
Feature에는 keynote랑 platforms state of the union 등이 있었고,
System framework쪽에는 Notification관련해서 바뀌는 점, Apple pay 등의 내용이 있습니다.

미디어 관련해서는 11개의 동영상이 있는데 우선 Advances in iOS Photography 같은 경우에는 기존에 쓰던 AVCaptureStillImageOutput이라고해서 이미지를 캡쳐할때 쓰던 클래스가 Deprecated 되면서 AVCapturePhotoOutput이라는 애가 생겼습니다. 그래서 그 부분관련해서 걔가 왜 생겼고, 어떻게 쓰면 된다에 관한 내용이고, Live Photo Editing and Raw Processing with core image 는 iphone 6s 부터 지원되는 live photo 기능이 그간에는 필터를 입히면 그냥 일반 이미지로 변경이 되었었는데, 이 부분을 필터를 입힐수 있고, 이번에 추가된 RAW 이미지등에도 CIFilter를 이용해서 필터를 입히는 부분에 대한 영상입니다.

AVcapturePhotoOutput 세션은 Session 501에서 못다한 예기를 추가적으로 더 하는 Chalk talk 세션입니다.

Graphics and Games쪽은 600번대 세션인데,
Go Live with ReplayKit 의 경우는 작년에 공개된 화면 녹화하는 ReplayKit이 이제 Streaming 기능까지 제공을 해서 그것에 관한 내용이고, What's New in Metal Part2의 경우는 재밌었던게 뉴럴넷 관련된 알고리즘들을 Metal을 이용해서 사용할 수 있게 해놓고 데모도 보여주고 있어서 재밌었던것 같습니다.

Advances in IOS Photography 세션은 새로 만들어진 AVCaptureOutput 인 AVCapturePhotoOutput에 대한 내용인데, 기존에 이미지를 캡쳐할때 사용을하던 AVCaptureStillImageOutput이 라이브포토 기능이라던지, ios10부터 지원되는 RAW 이미지파일, 그리고 캡쳐시에 Preview 사이즈로 가져온다던지 Wide Color 지원등이 안되기 때문에 만들어진 클래스인데요,

조금더 자세히 살펴보면, 이전에는 오른쪽과 같이 AVCaptureStillImageOutput을 이용해서 캡쳐를 했었어요. AVCaptureSession이라는거에 CaptureDevice를 DeviceInput으로 addInput으로 껴주고, AVCaptureStillImageOutput을 addOuput 함수를 통해서 session에 껴주고나면 블록으로 캡쳐된 이미지를 받아왔었습니다. 이 AVCaptureStillImageOutput이 AVCapturePhotoOutput으로 바뀐거구요.

AVCapturePhotoOutput을 쓰는것이 조금 바뀌긴 했는데, 그렇게 어렵진 않습니다.
우선 AVCapturePhotoOutput객체를 만들고, AVCapturePhotoSettings이라고 해서 예전에는 그냥 dictionary로 하던 세팅부분이 클래스로 바뀌었구요. 그래서 저 클래스를 세팅해주고 capturePhoto라는 함수에 세팅과 딜리게이트받을 객체를 넘겨주고 나면 이제 캡쳐와 관련된 이벤트가 발생할때마다 delegate의 함수가 호출되게 되는건거죠.

코드로 봐보면 하이레죨루션포토를 찍고싶다.하면 AVCapturePhotoSettings 객체를 만들고, isHighResolutionPhotoEnabled를 true 로 바꿔주고 capturePhoto에 setting파일과 delegate를 넘겨주면 됩니다.
플래쉬를 동작하고 싶으면 setting에 플레쉬모드를 바꿔서 capturePhoto에 넣어주면 되고, BGRA포멧으로 이미지를 받고싶다 하면 해당 세팅을 해서 capturePhoto에 물려주면 됩니다.

그래서 ios10에서는 기존에 쓰던 AVCaptureStillImageOutput이 deprecated 되었고, AVCaptureDevice쪽에 있던 flash관련된 변수들이 AVCapturePhotoOutput쪽으로 옮겨지고 deprecated 되었습니다.

더 자세한 내용은 SourceCode가 공개되어있으니 참고하시면 될것같습니다.

그럼 잘 쓰고있던 CaptureStillImageOutput을 왜 depreceted 되었냐 우선은 LIVE Photo를 캡쳐하는 기능을 지원하기 위해서인데요, Live Photo는 iphone6s 부터 제공되는 기능으로 촬영을 했을때 앞에 1.5초 촬영 후 1.5초 해서 3초간을 동영상으로 저장하는 기능입니다.

Live Photo는 순간이라는 사진적인 특성과, memory라는 영상적인 특성을 동시에 가지는데요, 우선 사진적인 특성은 기존에 non-live Photo와 동일한 퀄리티로 Still Image stabilization, Optical Image Stabilization 기능 모두 사용가능하고요. 영상적인 특성은 audio를 포함해서 1440*1080 또는 1290*960의 resolution으로 촬영이 되게 됩니다.

iOS10에는 추가적으로 live photo찍을때 video stabilization 기능이 추가되었고, 촬영할때 음악이 끊기던 부분을 수정했다고합니다. 얼마전에 구글에서 Motion stills 라고 Live Photo 영상을 stabilize 하고 플레이 해주는 앱을 냈던데, 며칠 안지나서 ios10에 stabilization기능이 추가되었다고 공개하는거 보니까 재밌다는 생각이 들더라구요.

아무튼 LivePhoto는 3초짜리 영상과 1.5초때의 이미지 한장으로 구성이 되는데요, 이걸 캡쳐하는 걸 조금더 살펴보면

AVCapturePhotoSetting 변수를 우선 만들어주고,
livePhotoMovieFileURL이라고 해서 동영상을 저장할 패스를 지정해주고요.
동영상에 meta정보를 저장하기 위해 AVMetadataItem이란걸 세팅해주고나서
capturePhoto함수에 setting 값을 넘기면 이제 Live Photo를 캡쳐할 수 있게됩니다.

이부분은 이제 1.5초대의 이미지를 저장하는 코드인거 같아요.

라이브포토를 보여주기위해선 PHLivePhotoView라는 애를 써서 보여주면 되는데요, PHImageManager라는 애가 이미지와 동영상을 가져와서 PHLivePhoto 객체로 만들어주고, 이 객체를 PHLivePhotoView가 보여주는 형태입니다.

live photo는 이번에 추가된 PHLivePhotoEditingContext라는 것을 이용해서 필터를 적용 할 수 있고, 조금 더 자세한 내용은 505세션 동영상을 참고하시면 됩니다.


다음으로는 RAW Format 이미지에 대한 내용입니다. CMOS Sensor는 Color Filter Array Sensor Array 두개의 레이어로 구성이 되어있는데, Color Filter Array라는건 빛을 통과시킬때 한 색상만 통과하게 해주는 필터입니다. Sensor Array에서는 필터를 통과한 빛을 센싱해서 값으로 바꿔주는 역할을 하고, 지금 보시면 R,B에 비해 G 가 두배로 많은걸 볼수있는데, 사람의 눈이 G값에 더 민감해서 두배로 센싱한다고 합니다. 아무튼 이렇게 센싱된 데이터자체는 RAW Format이라고 하고 ios10부터는 DNG란 포멧으로 이 값을 그대로 쓸수 있게 되었고, 이전에는 이 값을 가공한 상태인 JPEG상태의 데이터를 가져와서 쓰고있었습니다.

RAW 포멧을 캡쳐하는 방법은 앞에서 설명한 AVCapturePhotoSetting에서 세팅해주면 됩니다. 다만 다른점이 RAW 포멧은 후면카메라로 찍어야하고, SIS기능, highResolution기능은 꺼야합니다.

이부분은 딜리게이트 되는 부분 코드인데 AVCapturePhotoOutput의 dngPhotoDataRepresentation으로 data를 만들고 dng 파일로 만들어주는 코드입니다.


다음으로 설명드리는 부분은 preview image를 capture 하는 부분인데 기존에는 JPEG형태로 받아서 크기를 줄이는 과정을 거쳤는데, 이제는 capture시에 previewFormat에서 세팅을하면 previewImage를 capture할 수 있습니다.


다음으로는 Wide Color지원인데요, iPad Pro 9.7부터 Wide Color를 지원되고 ios9.3부터 Color management가 가능합니다.  이 그림은 기존의 sRGB Color space를 나타내고 있고, 여기서 조금 더 확장된것이 p3 Color space 인데 속색부분이 많이 늘어났죠. DCI-p3 Standard를 따르면서 기존과 같은 Gamma 값을 유지했다는데, 저는 아무튼 색 표현하는게 늘었구나 정도로 이해했습니다.

아무튼 그럼 우리는 capture를 할때 뭐를 해야하냐. 이부분은 AVCaptureSession이 거의 알아서 한다고 합니다. AVCaptureSession의 아래보이는 변수가 true로 세팅이 되어있으면, 오른쪽 위에 보이시는 것처럼 다양한 포멧의 데이터가 들어왔을때 실제 사용해야하는 포멧을 AVCaptureSession이 알아서 선택해준다고합니다.

다음으로 얘기드릴거는 Scene monitoring 에 대한 내용입니다.

아이폰에는 후면카메라에는 True Tone flash가 전면부에는 Retina flash가 제공되고, Still Image Stabilization이라고 해서 빛이 충분하지 않을때 Multiple image fusion capture라는 방식으로 이미지를 선명하게 하는 기능이 있습니다.

문제는 이 기능을 언제 사용하고 사용하지 않을지, 그리고 사용하고 있으면 사용하고 있다는것을 유저한테 알려줄수 있어야하는데요, 그 부분이 이번 AVCapturePhotoOutput에 추가되었습니다. 지금 보시는 API가 관련된 API이고요,

소스코드상에서 보면 AVCapturePhotoSetting을 만들어주고 플래쉬모드, SIS세팅을 해주고나서, photoOutput에 key value observing으로 해당 상태의 변화를 확인 할 수 있습니다.

마지막으로 Privacy Change에 관한 내용인데요,
ios7 부터 카메라나 엘범에 접근하려면 왼쪽과 같이 팝업으로 권한을 요청을하고 허락을해야 접근 할 수 있었는데요, ios10부터는 여기에 추가적으로 왜 그 권한을 요청하는지를 명시해야합니다. 명시하는 방법은 info.plist에 다음과 같은 값으로 세팅을 해주면 됩니다.

정리를 해보면 ios10에서는 wide Color라고해서 Display p3 Color space를 지원하고, 원본이미지 데이터인 RAW Format을 제공하며, Live Photo를 Editing하는 기능되었습니다.
그리고 오늘 설명드리지는 않았지만 화면을 녹화해주는 기능인 ReplayKit이 broadcasting을 지원하며, HLS도 Mpeg4를 지원하는등의 향상이 있습니다.

이상 발표를 마치겠습니다. 감사합니다.
